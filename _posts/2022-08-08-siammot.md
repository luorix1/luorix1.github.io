---
title: 'Siammot: Siamese multi-object tracking'
date: 2022-08-08
permalink: /posts/2022/08/blog-post-6/
tags:
  - Computer Vision
  - Machine Learning
  - Conference
---

[Siammot: Siamese multi-object tracking](http://arxiv.org/abs/2105.11595)]

### Faster-RCNN object detection

- Region Proposal Network (RPN)
- region-based detection network

### SiamMOT

- Faster-RCNN + region-based Siamese tracker
- Siamese tracker can model “instance-level motion”
    - comparing frames at $t'=t$ and $t'=t+\delta$

![Overview of SiamMOT architecture](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a7cc4599-8d90-4445-a1b1-2c43729f0c3d/Untitled.png)

Overview of SiamMOT architecture

### Motion modelling with Siamese tracker

- $i$ is detected at $t'=t$ in frame $\mathbf{I}^t$
- $R_i^t$ is “contextual window” around position of $i$ in frame $\mathbf{I}^t$
- $\tau$ is Siamese tracker
- adjacent frame is $I^{t+\delta}$

- define search region $S_i^{t+\delta}$ by expanding $R_i^t$ by fixed factor $r (> 1)$
- define $v_i^{t+\delta}$ as confidence of visibility for $i$ in frame $I^{t+\delta}$
- $f_{R_i}^t$ and $f_{S_i}^{t+\delta}$ are outputs from ROIAlign layer of Mask-RCNN (feature extraction)

- $(v_i^{t+\delta}, \tilde{R_i}^{t+\delta}) = \tau(f_{R_i}^t, f_{S_i}^{t+\delta}; \Theta)$

**Two possible ways to model motion!**

### Implicit Motion Model (IMM)

- concatenate $f_{R_i}^t$ and $f_{S_i}^{t+\delta}$ and feed into MLP
- predict
    - confidence of visibility $v_i$
    - relative location & scale change
        
        $\Delta x / w^t$, $\Delta y/h^t$, $log(\frac{w^{t+\delta}}{w^t})$, $log(\frac{h^{t+\delta}}{h^t})$
        
        ![Loss used for training IMM (* indicates GT values)](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/34ef081a-f002-4f10-8232-61e157963f24/Untitled.png)
        
        Loss used for training IMM (* indicates GT values)
        

### Explicit Motion Model (EMM)

- channel-wise cross-correlation operator
- CNN for generating visibility confidence map $v_i$
    - shows likelihood pixel contains the detected person
- CNN for generating location map $p_i$
    - outputs offset from each location to bbox corners

![Explicit Motion Model overview](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/95a262e6-f6b1-4340-96b7-09adce69288b/Untitled.png)

Explicit Motion Model overview

![Formula for penalty map](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc3734ee-1ebd-436b-89b1-ab8366aeea3b/Untitled.png)

Formula for penalty map

![Formula for decoding output from EMM](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/152dad72-6d3b-433a-b20e-b11bd95cdf1a/Untitled.png)

Formula for decoding output from EMM

![Loss used for training EMM](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a5765d26-ef92-415c-9aae-f1249de783cc/Untitled.png)

Loss used for training EMM