---
title: "Privacy safe representation learning via frequency filtering encoder"
collection: publications
category: conferences
permalink: /publication/2022-07-17-privacy
date: 2024-01-07
venue: 'The IJCAI-ECAI-22 Workshop on Artificial Intelligence Safety (AISafety 2022)'
paperurl: 'https://arxiv.org/pdf/2208.02482'
citation: 'Jeong, J., Cho, M., Benz, P., Hwang, J., Kim, J., Lee, S., & Kim, T. H. (2022). ”Privacy safe representation learning via frequency filtering encoder”. arXiv preprint arXiv:2208.02482. - The IJCAI-ECAI-22 Workshop on Artificial Intelligence Safety (AISafety 2022)'
---

Deep learning models are increasingly deployed in real-world applications. These models are often deployed on the server-side and receive user data in an information-rich representation to solve a specific task, such as image classification. Since images can contain sensitive information, which users might not be willing to share, privacy protection becomes increasingly important. Adversarial Representation Learning (ARL) is a common approach to train an encoder that runs on the client-side and obfuscates an image. It is assumed, that the obfuscated image can safely be transmitted and used for the task on the server without privacy concerns. However, in this work, we find that training a reconstruction attacker can successfully recover the original image of existing ARL methods. To this end, we introduce a novel ARL method enhanced through low-pass filtering, limiting the available information amount to be encoded in the frequency domain. Our experimental results reveal that our approach withstands reconstruction attacks while outperforming previous state-of-the-art methods regarding the privacy-utility trade-off. We further conduct a user study to qualitatively assess our defense of the reconstruction attack.